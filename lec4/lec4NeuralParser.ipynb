{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hRXpFR7btfPW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Parsing:\n",
        "  def __init__(self):\n",
        "    self.transitions = ['shift', 'left-arc', 'right-arc']\n",
        "\n",
        "  def init_config(self, sentence): # initially stack has root, buffer has all words and arcs are empty\n",
        "    stack = [0]\n",
        "    buffer = list(range(1, len(sentence)+1))\n",
        "    arcs = [] # will store [head, dependent] pairs\n",
        "    return stack, buffer, arcs\n",
        "\n",
        "  def moves(self, stack, buffer):\n",
        "    valid = [False, False, False] # return booleans for the actions - shift, left-arc and right arc\n",
        "    if len(buffer)>0:\n",
        "      valid[0]= True\n",
        "\n",
        "    if len(stack)>=2:\n",
        "      valid[2]= True\n",
        "      if stack[-2]!= 0:\n",
        "        valid[1]= True\n",
        "\n",
        "    return valid\n",
        "\n",
        "  def apply_moves(self, stack, buffer, arcs, act):\n",
        "    if act == 0: # shift\n",
        "      word = buffer.pop(0)\n",
        "      stack.append(word)\n",
        "\n",
        "    elif act == 1: # left-arc\n",
        "      dependent = stack.pop(-2)\n",
        "      head = stack[-1]\n",
        "      arcs.append((head, dependent))\n",
        "\n",
        "    elif act == 2: # right-arc\n",
        "      dependent = stack.pop(-1)\n",
        "      head = stack[-1]\n",
        "      arcs.append((head, dependent))\n",
        "\n",
        "    return stack, buffer, arcs"
      ],
      "metadata": {
        "id": "G15Nf5yiuLh9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ParsingModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes = 3):\n",
        "    super(ParsingModel, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "    self.input_size = embed_dim* 3 # top 2 stack and 1 top buffer\n",
        "    self.hidden = nn.Linear(self.input_size, hidden_dim)\n",
        "    self.output = nn.Linear(hidden_dim, num_classes)\n",
        "    self.activation = lambda x: torch.pow(x, 3) # cube activation by 'chen and manning' paper\n",
        "\n",
        "  def forward(self, feature_indices):\n",
        "    embeds = self.embedding(feature_indices)\n",
        "    embeds_flatten = embeds.view(embeds.size(0), -1)\n",
        "    x = self.activation(self.hidden(embeds_flatten)) # hidden transform + cube activation fnc\n",
        "    logits = self.output(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "zvP9pQE7wsT7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_extract(stack, buffer):\n",
        "  if len(stack)>0:\n",
        "    s1=stack[-1]\n",
        "  else:\n",
        "    s1= 0 # empty pos\n",
        "\n",
        "  if len(stack)>1:\n",
        "    s2=stack[-2]\n",
        "  else:\n",
        "    s2= 0\n",
        "\n",
        "  if len(buffer)>0:\n",
        "    b1 = buffer[0]\n",
        "  else:\n",
        "    b1 = 0\n",
        "\n",
        "  return torch.tensor([[s1, s2, b1]], dtype = torch.long)"
      ],
      "metadata": {
        "id": "H9974nB4yPVS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DnNaw0XWzHg8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}